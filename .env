########################################
# Colab-friendly .env for Prompta API  #
########################################

# FastAPI app settings
PROMPTA_APP_NAME=Prompta
PROMPTA_APP_VERSION=0.1.0
PROMPTA_DEBUG=true
PROMPTA_HOST=0.0.0.0
PROMPTA_PORT=8000

# Logging
PROMPTA_LOG_LEVEL=INFO   # DEBUG, INFO, WARNING, ERROR
PROMPTA_JSON_LOGS=true

# Auth (optional). Comma-separated tokens. Leave empty to disable auth.
# Example: PROMPTA_AUTH_BEARER_TOKENS=token123,another-token
PROMPTA_AUTH_BEARER_TOKENS=

# CORS (dev-friendly)
PROMPTA_CORS_ORIGINS=*

# Model / Inference
PROMPTA_MODEL_ID=openai/gpt-oss-20b

# Device placement:
# - cuda: full GPU (fastest; may OOM on small GPUs)
# - auto: smart layer offloading to CPU if needed (safer on Colab)
# - cpu: CPU only (slow)
PROMPTA_DEVICE_MAP=auto

# Torch dtype:
# - auto | float16 | bfloat16 | float32
# Use auto for safest behavior across GPU types
PROMPTA_TORCH_DTYPE=auto

# Hugging Face cache and auth (optional)
# Project-local cache keeps downloads inside the repo (ephemeral on Colab).
# You can switch to /content/.cache/huggingface to reuse across runs.
PROMPTA_HF_HOME=.hf
# If the model is gated/private, set your HF token
PROMPTA_HF_TOKEN=
